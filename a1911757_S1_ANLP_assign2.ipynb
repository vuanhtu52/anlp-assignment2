{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6730fecf",
   "metadata": {},
   "source": [
    "# Assignment 2 \n",
    "### Anh Tu Vu a1911757"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740d95cb",
   "metadata": {},
   "source": [
    "To view all related files used in this notebook, please refer to the full project at: https://github.com/vuanhtu52/anlp-assignment2.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df2aac6",
   "metadata": {},
   "source": [
    "## A. Tasks as specified for your team structure\n",
    "\n",
    "**One headings for each task.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89998f22",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3278470a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vuanhtu52/Desktop/Applied Natural Language Processing/Assignments/Assignment 2/anlp-assignment2/anlp_ass2/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AutoModel\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "from transformers import pipeline\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import neuralcoref\n",
    "import textwrap\n",
    "from gensim.models import Word2Vec\n",
    "import collections\n",
    "import string\n",
    "from tabulate import tabulate\n",
    "import allennlp_models\n",
    "from allennlp.predictors.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12bb8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/news_dataset.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2001503e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>topic</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307</td>\n",
       "      <td>Marlise Simons</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>architecture</td>\n",
       "      <td>PARIS  ?   When the Islamic State was about to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17292</td>\n",
       "      <td>Andy Newman</td>\n",
       "      <td>31/12/2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>art</td>\n",
       "      <td>Angels are everywhere in the Mu?iz family?s ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17298</td>\n",
       "      <td>Emma G. Fitzsimmons</td>\n",
       "      <td>2/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Finally. The Second Avenue subway opened in Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17311</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>3/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>WASHINGTON  ?   It?s   or   time for Republica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17339</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>5/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>For Megyn Kelly, the shift from Fox News to NB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id               author        date  year month         topic  \\\n",
       "0  17307       Marlise Simons   1/01/2017  2017     1  architecture   \n",
       "1  17292          Andy Newman  31/12/2016  2016    12           art   \n",
       "2  17298  Emma G. Fitzsimmons   2/01/2017  2017     1      business   \n",
       "3  17311           Carl Hulse   3/01/2017  2017     1      business   \n",
       "4  17339        Jim Rutenberg   5/01/2017  2017     1      business   \n",
       "\n",
       "                                             article  \n",
       "0  PARIS  ?   When the Islamic State was about to...  \n",
       "1  Angels are everywhere in the Mu?iz family?s ap...  \n",
       "2  Finally. The Second Avenue subway opened in Ne...  \n",
       "3  WASHINGTON  ?   It?s   or   time for Republica...  \n",
       "4  For Megyn Kelly, the shift from Fox News to NB...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0456503f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id         0\n",
       "author     6\n",
       "date       0\n",
       "year       0\n",
       "month      0\n",
       "topic      0\n",
       "article    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a24c00",
   "metadata": {},
   "source": [
    "No missing values in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d8679da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    return detect(text) == \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d762b06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:13<00:00, 76.23it/s]\n"
     ]
    }
   ],
   "source": [
    "articles = df[\"article\"].to_list()\n",
    "for article in tqdm(articles):\n",
    "    if not is_english(article):\n",
    "        print(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f20cac7",
   "metadata": {},
   "source": [
    "All articles are in English, so we do not have to discard any sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4a76d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy English model\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess(text, lemmatize=True):\n",
    "    \"\"\"Preprocess the article for word2vec model\"\"\"\n",
    "    text_cleaned = \"\"\n",
    "\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove ?\n",
    "    text_cleaned = text.replace(\"?\", \"\")\n",
    "\n",
    "    # Remove extra white spaces\n",
    "    text_cleaned = re.sub(\" +\", \" \", text_cleaned).strip()\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text_cleaned)\n",
    "    text_cleaned = \" \".join(tokens)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    text_cleaned = \" \".join([word for word in text_cleaned.split(\" \") if word not in stop_words])\n",
    "\n",
    "    # Lemmatize\n",
    "    if lemmatize:\n",
    "        doc = nlp(text_cleaned)\n",
    "        lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "        text_cleaned = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    return text_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "003694c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"article_cleaned\"] = df[\"article\"].progress_apply(preprocess)\n",
    "# df.to_csv(\"data/news_dataset_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d8e0a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>topic</th>\n",
       "      <th>article</th>\n",
       "      <th>article_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17307</td>\n",
       "      <td>Marlise Simons</td>\n",
       "      <td>1/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>architecture</td>\n",
       "      <td>PARIS  ?   When the Islamic State was about to...</td>\n",
       "      <td>pari islamic state drive ancient city palmyra ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17292</td>\n",
       "      <td>Andy Newman</td>\n",
       "      <td>31/12/2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>12</td>\n",
       "      <td>art</td>\n",
       "      <td>Angels are everywhere in the Mu?iz family?s ap...</td>\n",
       "      <td>angel everywhere muiz familys apartment bronx ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17298</td>\n",
       "      <td>Emma G. Fitzsimmons</td>\n",
       "      <td>2/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Finally. The Second Avenue subway opened in Ne...</td>\n",
       "      <td>finally . second avenue subway open new york c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17311</td>\n",
       "      <td>Carl Hulse</td>\n",
       "      <td>3/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>WASHINGTON  ?   It?s   or   time for Republica...</td>\n",
       "      <td>washington time republican . tumultuous decade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17339</td>\n",
       "      <td>Jim Rutenberg</td>\n",
       "      <td>5/01/2017</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>For Megyn Kelly, the shift from Fox News to NB...</td>\n",
       "      <td>megyn kelly , shift fox news nbc host daily da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id               author        date  year month         topic  \\\n",
       "0  17307       Marlise Simons   1/01/2017  2017     1  architecture   \n",
       "1  17292          Andy Newman  31/12/2016  2016    12           art   \n",
       "2  17298  Emma G. Fitzsimmons   2/01/2017  2017     1      business   \n",
       "3  17311           Carl Hulse   3/01/2017  2017     1      business   \n",
       "4  17339        Jim Rutenberg   5/01/2017  2017     1      business   \n",
       "\n",
       "                                             article  \\\n",
       "0  PARIS  ?   When the Islamic State was about to...   \n",
       "1  Angels are everywhere in the Mu?iz family?s ap...   \n",
       "2  Finally. The Second Avenue subway opened in Ne...   \n",
       "3  WASHINGTON  ?   It?s   or   time for Republica...   \n",
       "4  For Megyn Kelly, the shift from Fox News to NB...   \n",
       "\n",
       "                                     article_cleaned  \n",
       "0  pari islamic state drive ancient city palmyra ...  \n",
       "1  angel everywhere muiz familys apartment bronx ...  \n",
       "2  finally . second avenue subway open new york c...  \n",
       "3  washington time republican . tumultuous decade...  \n",
       "4  megyn kelly , shift fox news nbc host daily da...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/news_dataset_cleaned.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d83fc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"1\": {\n",
      "        \"question\": \"Who was the President during the conflict?\",\n",
      "        \"answer\": \"George W. Bush\",\n",
      "        \"article_id\": 17311\n",
      "    },\n",
      "    \"2\": {\n",
      "        \"question\": \"Who is the Senator of Colorado?\",\n",
      "        \"answer\": \"Cory Gardner\",\n",
      "        \"article_id\": 17311\n",
      "    },\n",
      "    \"3\": {\n",
      "        \"question\": \"What was the revolt?\",\n",
      "        \"answer\": \"Tea Party Revolt\",\n",
      "        \"article_id\": 17311\n",
      "    },\n",
      "    \"4\": {\n",
      "        \"question\": \"When did they get control back?\",\n",
      "        \"answer\": \"2010\",\n",
      "        \"article_id\": 17311\n",
      "    },\n",
      "    \"5\": {\n",
      "        \"question\": \"Where is the senior Republican from?\",\n",
      "        \"answer\": \"Oklahoma\",\n",
      "        \"article_id\": 17311\n",
      "    },\n",
      "    \"6\": {\n",
      "        \"question\": \"Who was the president during the Iraq War?\",\n",
      "        \"answer\": \"George W. Bush\",\n",
      "        \"article_id\": 17311\n",
      "    },\n",
      "    \"7\": {\n",
      "        \"question\": \"What amount did Fox News offer?\",\n",
      "        \"answer\": \"20 Million\",\n",
      "        \"article_id\": 17339\n",
      "    },\n",
      "    \"8\": {\n",
      "        \"question\": \"Where did Charlie Rose interview Kelly?\",\n",
      "        \"answer\": \"CBS Sunday Morning\",\n",
      "        \"article_id\": 17339\n",
      "    },\n",
      "    \"9\": {\n",
      "        \"question\": \"When did Andrew Lack take over?\",\n",
      "        \"answer\": \"2015\",\n",
      "        \"article_id\": 17339\n",
      "    },\n",
      "    \"10\": {\n",
      "        \"question\": \"What is Andrew Lack adding?\",\n",
      "        \"answer\": \"journalist schooled in the preferences and worldviews of the conservative Americans who helped elect Mr Trump\",\n",
      "        \"article_id\": 17339\n",
      "    },\n",
      "    \"11\": {\n",
      "        \"question\": \"When is Donald Trump inaugurated?\",\n",
      "        \"answer\": \"Jan. 20\",\n",
      "        \"article_id\": 17339\n",
      "    },\n",
      "    \"12\": {\n",
      "        \"question\": \"Who is a executive chairman?\",\n",
      "        \"answer\": \"Rupert Murdoch\",\n",
      "        \"article_id\": 17339\n",
      "    },\n",
      "    \"13\": {\n",
      "        \"question\": \"Who is an executive chairman of 21st Century Fox?\",\n",
      "        \"answer\": \"Rupert Murdoch\",\n",
      "        \"article_id\": 17339\n",
      "    },\n",
      "    \"14\": {\n",
      "        \"question\": \"Who is a novelist?\",\n",
      "        \"answer\": \"Douglas Brunt\",\n",
      "        \"article_id\": 17339\n",
      "    },\n",
      "    \"15\": {\n",
      "        \"question\": \"Where was the shooting?\",\n",
      "        \"answer\": \"Panzhihua\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"16\": {\n",
      "        \"question\": \"Who was embarrassed by the violence?\",\n",
      "        \"answer\": \"Xi Jinping\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"17\": {\n",
      "        \"question\": \"Chen Zhongshu is the head of what?\",\n",
      "        \"answer\": \"Panzhihua Land and Resources Bureau,\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"18\": {\n",
      "        \"question\": \"The shooting happened where?\",\n",
      "        \"answer\": \"Panzhihua\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"19\": {\n",
      "        \"question\": \"What was Chen Zhongshu the head of?\",\n",
      "        \"answer\": \"Panzhihua Land and Resources Bureau,\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"20\": {\n",
      "        \"question\": \"When did Zhang starting working there?\",\n",
      "        \"answer\": \"2006\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"21\": {\n",
      "        \"question\": \"Where is Panzhihua?\",\n",
      "        \"answer\": \"Sichuan Province\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"22\": {\n",
      "        \"question\": \"What is Panzhihua?\",\n",
      "        \"answer\": \"industrial city\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"23\": {\n",
      "        \"question\": \"When were they murdered?\",\n",
      "        \"answer\": \"June 2015\",\n",
      "        \"article_id\": 17300\n",
      "    },\n",
      "    \"24\": {\n",
      "        \"question\": \"What did they find him guilty of?\",\n",
      "        \"answer\": \"hate crimes resulting in death, obstruction of religion and firearms violations\",\n",
      "        \"article_id\": 17300\n",
      "    },\n",
      "    \"25\": {\n",
      "        \"question\": \"Who faces the death penalty?\",\n",
      "        \"answer\": \"Dylan Roof\",\n",
      "        \"article_id\": 17300\n",
      "    },\n",
      "    \"26\": {\n",
      "        \"question\": \"Who faces life in prison?\",\n",
      "        \"answer\": \"Dylan Roof\",\n",
      "        \"article_id\": 17300\n",
      "    },\n",
      "    \"27\": {\n",
      "        \"question\": \"When did he write?\",\n",
      "        \"answer\": \"Dec. 16\",\n",
      "        \"article_id\": 17300\n",
      "    },\n",
      "    \"28\": {\n",
      "        \"question\": \"Who is evil?\",\n",
      "        \"answer\": \"Dylan Roof\",\n",
      "        \"article_id\": 17300\n",
      "    },\n",
      "    \"29\": {\n",
      "        \"question\": \"When did the police officer shoot?\",\n",
      "        \"answer\": \"2015\",\n",
      "        \"article_id\": 17300\n",
      "    },\n",
      "    \"30\": {\n",
      "        \"question\": \"When was the coalition made?\",\n",
      "        \"answer\": \"2014\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"31\": {\n",
      "        \"question\": \"What did the Islamic State seize?\",\n",
      "        \"answer\": \"large areas in Iraq and neighboring Syria\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"32\": {\n",
      "        \"question\": \"When did the car bombing happen?\",\n",
      "        \"answer\": \"Monday\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"33\": {\n",
      "        \"question\": \"Where did the bombing occur?\",\n",
      "        \"answer\": \"a busy Baghdad market\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"34\": {\n",
      "        \"question\": \"Where did the attack occur?\",\n",
      "        \"answer\": \"Sadr City\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"35\": {\n",
      "        \"question\": \"How many died?\",\n",
      "        \"answer\": \"36\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"36\": {\n",
      "        \"question\": \"What was repeatedly attacked after the invasion?\",\n",
      "        \"answer\": \"Sadr City\",\n",
      "        \"article_id\": 17344\n",
      "    },\n",
      "    \"37\": {\n",
      "        \"question\": \"What did the gunman have?\",\n",
      "        \"answer\": \"a rifle\",\n",
      "        \"article_id\": 17314\n",
      "    },\n",
      "    \"38\": {\n",
      "        \"question\": \"Who is the spokesman?\",\n",
      "        \"answer\": \"Numan Kurtulmus\",\n",
      "        \"article_id\": 17314\n",
      "    },\n",
      "    \"39\": {\n",
      "        \"question\": \"Where is the gunman from?\",\n",
      "        \"answer\": \"Kyrgyzstan or elsewhere in Central Asia\",\n",
      "        \"article_id\": 17314\n",
      "    },\n",
      "    \"40\": {\n",
      "        \"question\": \"What happened in November?\",\n",
      "        \"answer\": \"A car bombing\",\n",
      "        \"article_id\": 17314\n",
      "    },\n",
      "    \"41\": {\n",
      "        \"question\": \"When did Kurtulmus say the attack happened?\",\n",
      "        \"answer\": \"just after midnight on Sunday morning\",\n",
      "        \"article_id\": 17314\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Get the test data\n",
    "test_data = {}\n",
    "with open(\"data/test_data.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "lines = [line.replace(\"\\n\", \"\") for line in lines]\n",
    "\n",
    "for line in lines:\n",
    "    id = line.split(\"|\")[0]\n",
    "    question = line.split(\"|\")[1]\n",
    "    answer = line.split(\"|\")[2]\n",
    "    article_id = int(line.split(\"|\")[3])\n",
    "    test_data[id] = {\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"article_id\": article_id\n",
    "    }\n",
    "\n",
    "print(json.dumps(test_data, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67214f76",
   "metadata": {},
   "source": [
    "### Functions used for evaluation later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22676d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Evaluation Script v2.0: https://rajpurkar.github.io/SQuAD-explorer/\n",
    "def normalize_answer(s):\n",
    "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "    return re.sub(regex, ' ', text)\n",
    "  def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "# Taken from Evaluation Script v2.0: https://rajpurkar.github.io/SQuAD-explorer/\n",
    "def get_tokens(s):\n",
    "  if not s: return []\n",
    "  return normalize_answer(s).split()\n",
    "\n",
    "# Taken from Evaluation Script v2.0: https://rajpurkar.github.io/SQuAD-explorer/\n",
    "def compute_f1(a_gold, a_pred):\n",
    "  gold_toks = get_tokens(a_gold)\n",
    "  pred_toks = get_tokens(a_pred)\n",
    "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "  num_same = sum(common.values())\n",
    "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "    return int(gold_toks == pred_toks)\n",
    "  if num_same == 0:\n",
    "    return 0\n",
    "  precision = 1.0 * num_same / len(pred_toks)\n",
    "  recall = 1.0 * num_same / len(gold_toks)\n",
    "  f1 = (2 * precision * recall) / (precision + recall)\n",
    "  return f1\n",
    "\n",
    "def compute_exact_match(a_gold, a_pred):\n",
    "  if a_gold == a_pred:\n",
    "    return 1\n",
    "  return 0\n",
    "\n",
    "# Taken from this repo: https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "def apk(actual, predicted, k=10):\n",
    "  \"\"\"\n",
    "  Computes the average precision at k.\n",
    "\n",
    "  This function computes the average prescision at k between two lists of\n",
    "  items.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  actual : list\n",
    "            A list of elements that are to be predicted (order doesn't matter)\n",
    "  predicted : list\n",
    "              A list of predicted elements (order does matter)\n",
    "  k : int, optional\n",
    "      The maximum number of predicted elements\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  score : double\n",
    "          The average precision at k over the input lists\n",
    "\n",
    "  \"\"\"\n",
    "  if len(predicted)>k:\n",
    "      predicted = predicted[:k]\n",
    "\n",
    "  score = 0.0\n",
    "  num_hits = 0.0\n",
    "\n",
    "  for i,p in enumerate(predicted):\n",
    "      if p in actual and p not in predicted[:i]:\n",
    "          num_hits += 1.0\n",
    "          score += num_hits / (i+1.0)\n",
    "\n",
    "  if not actual:\n",
    "      return 0.0\n",
    "\n",
    "  return score / min(len(actual), k)\n",
    "\n",
    "# Taken from this repo: https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "def mapk(actual, predicted, k=10):\n",
    "  \"\"\"\n",
    "  Computes the mean average precision at k.\n",
    "\n",
    "  This function computes the mean average prescision at k between two lists\n",
    "  of lists of items.\n",
    "\n",
    "  Parameters\n",
    "  ----------\n",
    "  actual : list\n",
    "            A list of lists of elements that are to be predicted \n",
    "            (order doesn't matter in the lists)\n",
    "  predicted : list\n",
    "              A list of lists of predicted elements\n",
    "              (order matters in the lists)\n",
    "  k : int, optional\n",
    "      The maximum number of predicted elements\n",
    "\n",
    "  Returns\n",
    "  -------\n",
    "  score : double\n",
    "          The mean average precision at k over the input lists\n",
    "\n",
    "  \"\"\"\n",
    "  return np.mean([apk(a,p,k) for a,p in zip(actual, predicted)])\n",
    "\n",
    "def mrr(actual, predicted):\n",
    "  reciprocal_ranks = []\n",
    "\n",
    "  for i in range(len(actual)):\n",
    "    truth = actual[i]\n",
    "    prediction = predicted[i]\n",
    "\n",
    "    for j, sent in enumerate(prediction):\n",
    "      if sent in truth:\n",
    "        reciprocal_ranks.append(1 / (j+1))\n",
    "        break\n",
    "         \n",
    "  if sum(reciprocal_ranks) == 0:\n",
    "     return 0\n",
    "         \n",
    "  return sum(reciprocal_ranks) / len(reciprocal_ranks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0c918",
   "metadata": {},
   "source": [
    "### Classic Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6434f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train word2vec model on all the articles\n",
    "w2v_model = Word2Vec(df[\"article_cleaned\"].apply(word_tokenize).to_list(), min_count=1)\n",
    "\n",
    "def text_to_vector(text):\n",
    "    \"\"\"Convert a sentence to vector using word2vec\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return sum(w2v_model.wv[token] for token in tokens) / len(tokens)\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "        # Calculate the dot product between v1 and v2\n",
    "        dot_product = np.dot(v1, v2)\n",
    "\n",
    "        # Calculate |v1| . |v2|\n",
    "        magnitude = np.sqrt(np.sum(v1 ** 2) * np.sum(v2 ** 2))\n",
    "\n",
    "        return dot_product / magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1811ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "def sort_sentences(question, article):\n",
    "    \"\"\"Split the article into sentences and sort them based on relevance to the question\"\"\"\n",
    "    # Preprocess the article\n",
    "    article = article.replace(\"?\", \"\")\n",
    "    article = re.sub(\" +\", \" \", article).strip()\n",
    "\n",
    "    # Resolve coreferences\n",
    "    doc = nlp(article)\n",
    "    article = nlp(doc._.coref_resolved).text\n",
    "\n",
    "    # Split to sentences\n",
    "    sentences = sent_tokenize(article)\n",
    "\n",
    "    # Calculate cosine similarity between each sentence and question\n",
    "    similarities = []\n",
    "    question_vector = text_to_vector(preprocess(question))\n",
    "    for sent in sentences:\n",
    "        try:\n",
    "            sent_vector = text_to_vector(preprocess(sent))\n",
    "        except Exception:\n",
    "            similarities.append(0)\n",
    "            continue\n",
    "        similarity = cosine_similarity(question_vector, sent_vector)\n",
    "        similarities.append(similarity)\n",
    "\n",
    "    # Save everything to a dataframe\n",
    "    sent_df = pd.DataFrame()\n",
    "    sent_df[\"id\"] = [_id for _id in range(len(sentences))]\n",
    "    sent_df[\"sentence\"] = sentences\n",
    "    sent_df[\"similarity\"] = similarities\n",
    "\n",
    "    # Sort based on similarity\n",
    "    sent_df = sent_df.sort_values(\"similarity\", ascending=False)\n",
    "\n",
    "    return sent_df[\"sentence\"].to_list()\n",
    "\n",
    "def answer_question_classic(question, article):\n",
    "    \"\"\"Return a snippet of text from the article as answer to the question\"\"\"\n",
    "    # Sort the sentences from the article based on relevance to the question\n",
    "    sentences = sort_sentences(question=question, article=article)\n",
    "\n",
    "    # Extract answer from entities based on question type\n",
    "    for sent in sentences:\n",
    "        doc = nlp(sent)\n",
    "        for ent in doc.ents:\n",
    "            if \"who\" in question.lower() and ent.label_ == \"PERSON\":\n",
    "                return ent.text\n",
    "            if \"when\" in question.lower() and ent.label_ == \"DATE\":\n",
    "                return ent.text\n",
    "            if \"where\" in question.lower() and ent.label_ == \"LOC\":\n",
    "                return ent.text\n",
    "            if \"how many\" in question.lower() and ent.label_ == \"CARDINAL\":\n",
    "                return ent.text\n",
    "            if \"what amount\" in question.lower() and ent.label_ == \"CARDINAL\":\n",
    "                return ent.text\n",
    "            if \"what\" in question.lower() and ent.label_ in [\"ORG\", \"LOC\", \"NORP\", \"GPE\"]:\n",
    "                return ent.text\n",
    "            \n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56b11210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 41/41 [02:34<00:00,  3.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1: 0.18292682926829268\n",
      "Average exact match scores: 0.12195121951219512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on custom test set\n",
    "f1_scores = []\n",
    "em_scores = []\n",
    "\n",
    "for _id in tqdm(test_data, desc=\"Evaluating\"):\n",
    "    question = test_data[_id][\"question\"]\n",
    "    article = df.loc[df[\"id\"] == test_data[_id][\"article_id\"], \"article\"].values[0]\n",
    "    answer_gold = test_data[_id][\"answer\"]\n",
    "\n",
    "    answer_predicted = answer_question_classic(question=question, article=article)\n",
    "    f1_scores.append(compute_f1(a_gold=answer_gold, a_pred=answer_predicted))\n",
    "    em_scores.append(compute_exact_match(a_gold=answer_gold, a_pred=answer_predicted))\n",
    "\n",
    "print(f\"Average f1: {sum(f1_scores) / len(f1_scores)}\")\n",
    "print(f\"Average exact match scores: {sum(em_scores) / len(em_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00f6a97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 41/41 [02:28<00:00,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   MAP@5    MAP@10    MAP@15       MRR\n",
      "--------  --------  --------  --------\n",
      "0.143171  0.173056  0.184014  0.299168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate how well the system finds the relevant sentences on custom test set\n",
    "\n",
    "truths = []\n",
    "predictions = []\n",
    "\n",
    "for _id in tqdm(test_data, desc=\"Evaluating\"):\n",
    "    # Extraction information from test dataset\n",
    "    question = test_data[_id][\"question\"]\n",
    "    article = df.loc[df[\"id\"] == test_data[_id][\"article_id\"], \"article\"].values[0]\n",
    "    answer_gold = test_data[_id][\"answer\"]\n",
    "\n",
    "    # Get the ranked sentences from the system\n",
    "    sentences = sort_sentences(question=question, article=article)\n",
    "    prediction = sentences\n",
    "\n",
    "    # Get the relevant sentences \n",
    "    truth = []\n",
    "    for sent in sentences:\n",
    "        if answer_gold.lower() in sent.lower():\n",
    "            truth.append(sent) \n",
    "\n",
    "    truths.append(truth)\n",
    "    predictions.append(prediction)  \n",
    "\n",
    "print(tabulate([\n",
    "    [\n",
    "        mapk(actual=truths, predicted=predictions, k=5),\n",
    "        mapk(actual=truths, predicted=predictions, k=10),\n",
    "        mapk(actual=truths, predicted=predictions, k=15),\n",
    "        mrr(actual=truths, predicted=predictions)\n",
    "    ]\n",
    "], headers=[\"MAP@5\", \"MAP@10\", \"MAP@15\", \"MRR\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e354330",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19029/19029 [16:09<00:00, 19.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train the word2vec model on squad train dataset\n",
    "\n",
    "# with open(\"squad/train-v2.0.json\", \"r\") as f:\n",
    "#     squad_train = json.load(f)\n",
    "\n",
    "# articles = []\n",
    "# for article in squad_train[\"data\"]:\n",
    "#     for paragraph in article[\"paragraphs\"]:\n",
    "#         articles.append(paragraph[\"context\"])\n",
    "\n",
    "# articles = list(set(articles))\n",
    "# for i, article in tqdm(enumerate(articles), total=len(articles)):\n",
    "#     articles[i] = preprocess(article)\n",
    "\n",
    "# w2v_model_2 = Word2Vec(articles, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbd9399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model_2.save(\"squad/models/w2v_squad.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ed90f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_2 = Word2Vec.load(\"squad/models/w2v_squad.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62294ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_article(text):\n",
    "    \"\"\"Preprocessing the articles before feeding into the models\"\"\"\n",
    "    text_cleaned = \"\"\n",
    "\n",
    "    # Remove ?\n",
    "    text_cleaned = text.replace(\"?\", \"\")\n",
    "\n",
    "    # Remove extra white spaces\n",
    "    text_cleaned = re.sub(\" +\", \" \", text_cleaned).strip()\n",
    "\n",
    "    return text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "863e61f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Articles: 100%|██████████| 35/35 [1:04:37<00:00, 110.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on the squad dev set\n",
    "\n",
    "# f1_scores = []\n",
    "# em_scores = []\n",
    "\n",
    "# with open(\"squad/dev-v2.0.json\", \"r\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# ids = []\n",
    "# answers = []\n",
    "# for article in tqdm(data[\"data\"], desc=\"Articles\"):\n",
    "#         for paragraph in article[\"paragraphs\"]:\n",
    "#             context = paragraph[\"context\"]\n",
    "#             for qa in paragraph[\"qas\"]:\n",
    "#                 try:\n",
    "#                     answer_predicted = answer_question_classic(question=qa[\"question\"], article=preprocess_article(context))\n",
    "#                     answers.append(answer_predicted)\n",
    "#                     ids.append(qa[\"id\"])\n",
    "#                 except Exception:\n",
    "#                      answers.append(\"\")\n",
    "#                      ids.append(qa[\"id\"])\n",
    "\n",
    "\n",
    "# predictions = {}\n",
    "# for i, _ in enumerate(ids):\n",
    "#     predictions[ids[i]] = answers[i]\n",
    "\n",
    "# with open(\"squad/predictions/classic.json\", \"w\") as f:\n",
    "#     json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "451bbe4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"exact\": 34.0604733428788,\n",
      "  \"f1\": 34.75574425638583,\n",
      "  \"total\": 11873,\n",
      "  \"HasAns_exact\": 4.301619433198381,\n",
      "  \"HasAns_f1\": 5.694155120794402,\n",
      "  \"HasAns_total\": 5928,\n",
      "  \"NoAns_exact\": 63.734230445752736,\n",
      "  \"NoAns_f1\": 63.734230445752736,\n",
      "  \"NoAns_total\": 5945\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'squad/evaluate-v2.0.py', 'squad/dev-v2.0.json', 'squad/predictions/classic.json'], returncode=0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on squad dev set\n",
    "subprocess.run([\"python\", \"squad/evaluate-v2.0.py\"] + [\"squad/dev-v2.0.json\", \"squad/predictions/classic.json\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8d33d",
   "metadata": {},
   "source": [
    "### BiDAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e034689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vuanhtu52/Desktop/Applied Natural Language Processing/Assignments/Assignment 2/anlp-assignment2/anlp_ass2/lib/python3.7/site-packages/huggingface_hub/file_download.py:654: FutureWarning: 'cached_download' is the legacy way to download files from the HF hub, please consider upgrading to 'hf_hub_download'\n",
      "  FutureWarning,\n",
      "Fetching 11 files: 100%|██████████| 11/11 [00:00<00:00, 35246.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "predictor = Predictor.from_path(\"hf://allenai/bidaf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48de50bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 41/41 [00:10<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1: 0.6053832777658673\n",
      "Average exact match scores: 0.36585365853658536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on custom test set\n",
    "f1_scores = []\n",
    "em_scores = []\n",
    "\n",
    "for _id in tqdm(test_data, desc=\"Evaluating\"):\n",
    "    question = test_data[_id][\"question\"]\n",
    "    article = df.loc[df[\"id\"] == test_data[_id][\"article_id\"], \"article\"].values[0]\n",
    "    answer_gold = test_data[_id][\"answer\"]\n",
    "\n",
    "    answer_predicted = predictor.predict_json({\n",
    "        \"passage\": preprocess_article(article),\n",
    "        \"question\": question\n",
    "    })[\"best_span_str\"]\n",
    "\n",
    "    f1_scores.append(compute_f1(a_gold=answer_gold, a_pred=answer_predicted))\n",
    "    em_scores.append(compute_exact_match(a_gold=answer_gold, a_pred=answer_predicted))\n",
    "\n",
    "print(f\"Average f1: {sum(f1_scores) / len(f1_scores)}\")\n",
    "print(f\"Average exact match scores: {sum(em_scores) / len(em_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61cb7581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the predictions on squad dataset\n",
    "\n",
    "# with open(\"squad/dev-v2.0.json\", \"r\") as f:\n",
    "#         data = json.load(f)\n",
    "# ids = []\n",
    "# answers = []\n",
    "# for article in tqdm(data[\"data\"], desc=\"Articles\"):\n",
    "#     for paragraph in article[\"paragraphs\"]:\n",
    "#         context = paragraph[\"context\"]\n",
    "#         for qa in paragraph[\"qas\"]:\n",
    "#             answer_predicted = predictor.predict_json({\n",
    "#                  \"passage\": preprocess_article(context),\n",
    "#                 \"question\": qa[\"question\"]\n",
    "#             })[\"best_span_str\"]\n",
    "#             answers.append(answer_predicted)\n",
    "#             ids.append(qa[\"id\"])\n",
    "\n",
    "# predictions = {}\n",
    "# for i, _ in enumerate(ids):\n",
    "#     predictions[ids[i]] = answers[i]\n",
    "\n",
    "# with open(\"squad/predictions/bidaf.json\", \"w\") as f:\n",
    "#     json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fdac7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"exact\": 33.95940368904237,\n",
      "  \"f1\": 38.74344064147275,\n",
      "  \"total\": 11873,\n",
      "  \"HasAns_exact\": 68.01619433198381,\n",
      "  \"HasAns_f1\": 77.59798764106039,\n",
      "  \"HasAns_total\": 5928,\n",
      "  \"NoAns_exact\": 0.0,\n",
      "  \"NoAns_f1\": 0.0,\n",
      "  \"NoAns_total\": 5945\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'squad/evaluate-v2.0.py', 'squad/dev-v2.0.json', 'squad/predictions/bidaf.json'], returncode=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on squad dataset\n",
    "subprocess.run([\"python\", \"squad/evaluate-v2.0.py\"] + [\"squad/dev-v2.0.json\", \"squad/predictions/bidaf.json\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe06173",
   "metadata": {},
   "source": [
    "### SpanBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0790fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"mrm8488/spanbert-finetuned-squadv2\",\n",
    "    tokenizer=\"mrm8488/spanbert-finetuned-squadv2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "873d9acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 41/41 [00:44<00:00,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1: 0.6110917537746806\n",
      "Average exact match scores: 0.4634146341463415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on custom test set\n",
    "f1_scores = []\n",
    "em_scores = []\n",
    "\n",
    "for _id in tqdm(test_data, desc=\"Evaluating\"):\n",
    "    question = test_data[_id][\"question\"]\n",
    "    answer_gold = test_data[_id][\"answer\"]\n",
    "    article = df.loc[df[\"id\"] == test_data[_id][\"article_id\"], \"article\"].values[0]\n",
    "\n",
    "    answer_predicted = qa_pipeline({\n",
    "        \"context\": preprocess_article(article),\n",
    "        \"question\": question\n",
    "    })[\"answer\"]\n",
    "    \n",
    "    f1_scores.append(compute_f1(a_gold=answer_gold, a_pred=answer_predicted))\n",
    "    em_scores.append(compute_exact_match(a_gold=answer_gold, a_pred=answer_predicted))\n",
    "    \n",
    "print(f\"Average f1: {sum(f1_scores) / len(f1_scores)}\")\n",
    "print(f\"Average exact match scores: {sum(em_scores) / len(em_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd05a28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the predictions on squad dataset\n",
    "\n",
    "# with open(\"squad/dev-v2.0.json\", \"r\") as f:\n",
    "#         data = json.load(f)\n",
    "# ids = []\n",
    "# answers = []\n",
    "# for article in tqdm(data[\"data\"], desc=\"Articles\"):\n",
    "#     for paragraph in article[\"paragraphs\"]:\n",
    "#         context = paragraph[\"context\"]\n",
    "#         for qa in paragraph[\"qas\"]:\n",
    "#             answer_predicted = qa_pipeline({\n",
    "#                 \"context\": preprocess_article(context),\n",
    "#                 \"question\": qa[\"question\"]\n",
    "#             })[\"answer\"]\n",
    "#             answers.append(answer_predicted)\n",
    "#             ids.append(qa[\"id\"])\n",
    "\n",
    "# predictions = {}\n",
    "# for i, _ in enumerate(ids):\n",
    "#     predictions[ids[i]] = answers[i]\n",
    "\n",
    "# with open(\"squad/predictions/spanbert.json\", \"w\") as f:\n",
    "#     json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03173147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{\n",
      "  \"exact\": 41.56489514023414,\n",
      "  \"f1\": 45.378712108463006,\n",
      "  \"total\": 11873,\n",
      "  \"HasAns_exact\": 83.1140350877193,\n",
      "  \"HasAns_f1\": 90.75260608363384,\n",
      "  \"HasAns_total\": 5928,\n",
      "  \"NoAns_exact\": 0.1345668629100084,\n",
      "  \"NoAns_f1\": 0.1345668629100084,\n",
      "  \"NoAns_total\": 5945\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'squad/evaluate-v2.0.py', 'squad/dev-v2.0.json', 'squad/predictions/spanbert.json'], returncode=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on squad dataset\n",
    "subprocess.run([\"python\", \"squad/evaluate-v2.0.py\"] + [\"squad/dev-v2.0.json\", \"squad/predictions/spanbert.json\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68361e60",
   "metadata": {},
   "source": [
    "### BERT Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25ccb339",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"deepset/bert-base-cased-squad2\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "bert_base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_base_model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86e1e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_bert_base(question, article):\n",
    "    # Encode the question and passage using the tokenizer\n",
    "    inputs = bert_base_tokenizer.encode_plus(question, article, return_tensors=\"pt\", max_length=512, truncation=True, truncation_strategy=\"only_second\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    token_type_ids = inputs[\"token_type_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Pass the encoded input through the model\n",
    "    outputs = bert_base_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, return_dict=True)\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    # Decode the predicted start and end positions to get the answer\n",
    "    start_index = torch.argmax(start_logits)\n",
    "    end_index = torch.argmax(end_logits) + 1\n",
    "\n",
    "    # input_tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    answer_tokens = input_ids[0][start_index:end_index]\n",
    "    answer = bert_base_tokenizer.decode(answer_tokens)\n",
    "\n",
    "    # Skip over any tokens before the start position or after the end position\n",
    "    for token in answer_tokens:\n",
    "        if token == bert_base_tokenizer.cls_token_id:\n",
    "            start_index += 1\n",
    "        elif token == bert_base_tokenizer.sep_token_id:\n",
    "            end_index -= 1\n",
    "    answer_tokens = input_ids[0][start_index:end_index]\n",
    "\n",
    "    # Decode the answer tokens to get the final answer\n",
    "    answer = bert_base_tokenizer.decode(answer_tokens)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "58a1b641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 41/41 [00:13<00:00,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1: 0.220709262307347\n",
      "Average exact match scores: 0.1951219512195122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate answers on custom test set\n",
    "f1_scores = []\n",
    "em_scores = []\n",
    "\n",
    "for _id in tqdm(test_data, desc=\"Evaluating\"):\n",
    "    question = test_data[_id][\"question\"]\n",
    "    answer_gold = test_data[_id][\"answer\"]\n",
    "    article = df.loc[df[\"id\"] == test_data[_id][\"article_id\"], \"article\"].values[0]\n",
    "\n",
    "    answer_predicted = answer_question_bert_base(question=question, article=preprocess_article(article))\n",
    "    f1_scores.append(compute_f1(a_gold=answer_gold, a_pred=answer_predicted))\n",
    "    em_scores.append(compute_exact_match(a_gold=answer_gold, a_pred=answer_predicted))\n",
    "    \n",
    "print(f\"Average f1: {sum(f1_scores) / len(f1_scores)}\")\n",
    "print(f\"Average exact match scores: {sum(em_scores) / len(em_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba9c8576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Articles: 100%|██████████| 35/35 [26:46<00:00, 45.90s/it]\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on squad dataset\n",
    "\n",
    "# with open(\"squad/dev-v2.0.json\", \"r\") as f:\n",
    "#         data = json.load(f)\n",
    "# ids = []\n",
    "# answers = []\n",
    "# for article in tqdm(data[\"data\"], desc=\"Articles\"):\n",
    "#     for paragraph in article[\"paragraphs\"]:\n",
    "#         context = paragraph[\"context\"]\n",
    "#         for qa in paragraph[\"qas\"]:\n",
    "#             answer_predicted = answer_question_bert_base(question=qa[\"question\"], article=preprocess_article(context))\n",
    "#             answers.append(answer_predicted)\n",
    "#             ids.append(qa[\"id\"])\n",
    "\n",
    "# predictions = {}\n",
    "# for i, _ in enumerate(ids):\n",
    "#     predictions[ids[i]] = answers[i]\n",
    "\n",
    "# with open(\"squad/predictions/bert_base.json\", \"w\") as f:\n",
    "#     json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "824e05e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{\n",
      "  \"exact\": 67.21131980122968,\n",
      "  \"f1\": 71.39165741443425,\n",
      "  \"total\": 11873,\n",
      "  \"HasAns_exact\": 58.232118758434545,\n",
      "  \"HasAns_f1\": 66.60478213251974,\n",
      "  \"HasAns_total\": 5928,\n",
      "  \"NoAns_exact\": 76.16484440706476,\n",
      "  \"NoAns_f1\": 76.16484440706476,\n",
      "  \"NoAns_total\": 5945\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'squad/evaluate-v2.0.py', 'squad/dev-v2.0.json', 'squad/predictions/bert_base.json'], returncode=0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on squad dataset\n",
    "subprocess.run([\"python\", \"squad/evaluate-v2.0.py\"] + [\"squad/dev-v2.0.json\", \"squad/predictions/bert_base.json\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5926c8cc",
   "metadata": {},
   "source": [
    "### DISTILBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fd667608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"distilbert-base-cased-distilled-squad\"\n",
    "distilbert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "distilbert_model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05dd12ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_distilbert(question, article):\n",
    "    # Encode the question and passage using the tokenizer\n",
    "    inputs = distilbert_tokenizer.encode_plus(question, article, return_tensors=\"pt\", max_length=512, truncation=True, truncation_strategy=\"only_second\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "    # Pass the encoded input through the QA model\n",
    "    outputs = distilbert_model(input_ids, attention_mask, return_dict=True)\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "    # Decode the predicted start and end positions to get the answer\n",
    "    start_index = torch.argmax(start_logits)\n",
    "    end_index = torch.argmax(end_logits) + 1\n",
    "\n",
    "    # Skip over any tokens before the start position or after the end position\n",
    "    for j, token_id in enumerate(input_ids[0]):\n",
    "        if j < start_index or j >= end_index:\n",
    "            input_ids[0][j] = distilbert_tokenizer.pad_token_id\n",
    "\n",
    "    # Decode the answer from the corresponding tokens\n",
    "    answer_tokens = input_ids[0][start_index:end_index]\n",
    "    answer = distilbert_tokenizer.decode(answer_tokens)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4a8f095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 41/41 [00:07<00:00,  5.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1: 0.3310104529616725\n",
      "Average exact match scores: 0.24390243902439024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "f1_scores = []\n",
    "em_scores = []\n",
    "\n",
    "for _id in tqdm(test_data, desc=\"Evaluating\"):\n",
    "    question = test_data[_id][\"question\"]\n",
    "    answer_gold = test_data[_id][\"answer\"]\n",
    "    article = df.loc[df[\"id\"] == test_data[_id][\"article_id\"], \"article\"].values[0]\n",
    "\n",
    "    answer_predicted = answer_question_distilbert(question=question, article=preprocess_article(article))\n",
    "    f1_scores.append(compute_f1(a_gold=answer_gold, a_pred=answer_predicted))\n",
    "    em_scores.append(compute_exact_match(a_gold=answer_gold, a_pred=answer_predicted))\n",
    "    \n",
    "print(f\"Average f1: {sum(f1_scores) / len(f1_scores)}\")\n",
    "print(f\"Average exact match scores: {sum(em_scores) / len(em_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f1b402c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on squad dataset\n",
    "\n",
    "# with open(\"squad/dev-v2.0.json\", \"r\") as f:\n",
    "#         data = json.load(f)\n",
    "# ids = []\n",
    "# answers = []\n",
    "# for article in tqdm(data[\"data\"], desc=\"Articles\"):\n",
    "#     for paragraph in article[\"paragraphs\"]:\n",
    "#         context = paragraph[\"context\"]\n",
    "#         for qa in paragraph[\"qas\"]:\n",
    "#             answer_predicted = answer_question_distilbert(question=qa[\"question\"], article=preprocess_article(context))\n",
    "#             answers.append(answer_predicted)\n",
    "#             ids.append(qa[\"id\"])\n",
    "\n",
    "# predictions = {}\n",
    "# for i, _ in enumerate(ids):\n",
    "#     predictions[ids[i]] = answers[i]\n",
    "\n",
    "# with open(\"squad/predictions/distilbert.json\", \"w\") as f:\n",
    "#     json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1137f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{\n",
      "  \"exact\": 36.64617198686094,\n",
      "  \"f1\": 41.272836055899646,\n",
      "  \"total\": 11873,\n",
      "  \"HasAns_exact\": 71.17071524966262,\n",
      "  \"HasAns_f1\": 80.43731148645352,\n",
      "  \"HasAns_total\": 5928,\n",
      "  \"NoAns_exact\": 2.220353238015139,\n",
      "  \"NoAns_f1\": 2.220353238015139,\n",
      "  \"NoAns_total\": 5945\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'squad/evaluate-v2.0.py', 'squad/dev-v2.0.json', 'squad/predictions/distilbert.json'], returncode=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on squad dataset\n",
    "subprocess.run([\"python\", \"squad/evaluate-v2.0.py\"] + [\"squad/dev-v2.0.json\", \"squad/predictions/distilbert.json\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ee3836",
   "metadata": {},
   "source": [
    "### BERT Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35f31ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "bert_large_model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "bert_large_tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c1b7bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question_bert_large(question, article):\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the encode text, treating them as a question, answer_text pair.\n",
    "    input_ids = bert_large_tokenizer.encode(question, article, max_length=512)\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(bert_large_tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Run our example question through the model.\n",
    "    model_scores = bert_large_model.forward(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                 token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "    \n",
    "    start_scores = model_scores.start_logits\n",
    "    end_scores = model_scores.end_logits\n",
    "    \n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = bert_large_tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        # print(tokens[i])\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9720366b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 41/41 [00:44<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average f1: 0.433925271882589\n",
      "Average exact match scores: 0.04878048780487805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "f1_scores = []\n",
    "em_scores = []\n",
    "\n",
    "for _id in tqdm(test_data, desc=\"Evaluating\"):\n",
    "    question = test_data[_id][\"question\"]\n",
    "    answer_gold = test_data[_id][\"answer\"]\n",
    "    article = df.loc[df[\"id\"] == test_data[_id][\"article_id\"], \"article\"].values[0]\n",
    "\n",
    "    answer_predicted = answer_question_bert_large(question=question, article=preprocess_article(article))\n",
    "    f1_scores.append(compute_f1(a_gold=answer_gold, a_pred=answer_predicted))\n",
    "    em_scores.append(compute_exact_match(a_gold=answer_gold, a_pred=answer_predicted))\n",
    "    \n",
    "print(f\"Average f1: {sum(f1_scores) / len(f1_scores)}\")\n",
    "print(f\"Average exact match scores: {sum(em_scores) / len(em_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2285cd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on squad dataset\n",
    "\n",
    "# with open(\"squad/dev-v2.0.json\", \"r\") as f:\n",
    "#         data = json.load(f)\n",
    "# ids = []\n",
    "# answers = []\n",
    "# for article in tqdm(data[\"data\"], desc=\"Articles\"):\n",
    "#     for paragraph in article[\"paragraphs\"]:\n",
    "#         context = paragraph[\"context\"]\n",
    "#         for qa in paragraph[\"qas\"]:\n",
    "#             answer_predicted = answer_question_bert_large(question=qa[\"question\"], article=preprocess_article(context))\n",
    "#             answers.append(answer_predicted)\n",
    "#             ids.append(qa[\"id\"])\n",
    "\n",
    "# predictions = {}\n",
    "# for i, _ in enumerate(ids):\n",
    "#     predictions[ids[i]] = answers[i]\n",
    "\n",
    "# with open(\"squad/predictions/bert_large.json\", \"w\") as f:\n",
    "#     json.dump(predictions, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "517817dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "{\n",
      "  \"exact\": 37.993767371346756,\n",
      "  \"f1\": 42.75583509830704,\n",
      "  \"total\": 11873,\n",
      "  \"HasAns_exact\": 75.86032388663968,\n",
      "  \"HasAns_f1\": 85.39811574261124,\n",
      "  \"HasAns_total\": 5928,\n",
      "  \"NoAns_exact\": 0.23549201009251472,\n",
      "  \"NoAns_f1\": 0.23549201009251472,\n",
      "  \"NoAns_total\": 5945\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python', 'squad/evaluate-v2.0.py', 'squad/dev-v2.0.json', 'squad/predictions/bert_large.json'], returncode=0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on squad dataset\n",
    "subprocess.run([\"python\", \"squad/evaluate-v2.0.py\"] + [\"squad/dev-v2.0.json\", \"squad/predictions/bert_large.json\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d4563",
   "metadata": {},
   "source": [
    "### User Interaction System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44ba07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_app():\n",
    "    while True:\n",
    "        # Ask user to select a model\n",
    "        print(\"Please select a model to answer your question: \\n1. Classic model\\n2. BiDAF\\n3. SpanBERT\\n4. BERT base\\n5. DISTILBERT\\n6.BERT large\")\n",
    "        model_id = \"\"\n",
    "        while True:\n",
    "            model_id = input(\"Enter model id: \")\n",
    "\n",
    "            if model_id == \"quit\":\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                model_id = int(model_id)\n",
    "            except Exception:\n",
    "                print(\"Wrong id. Enter again: \")\n",
    "                continue\n",
    "\n",
    "            if int(model_id) >= 1 and int(model_id) <= 6:\n",
    "                break\n",
    "            else:\n",
    "                print(\"Wrong id. Enter again: \")\n",
    "\n",
    "        if model_id == \"quit\":\n",
    "            break\n",
    "\n",
    "        print(f\"Model selected: {model_id}\\n\")\n",
    "\n",
    "\n",
    "        # Ask user to enter article number\n",
    "        # article_id = input(\"Please select an article id: \")\n",
    "        article_id = \"\"\n",
    "        article = \"\"\n",
    "        while True:\n",
    "            article_id = input(\"Please select an article id: \")\n",
    "\n",
    "            if article_id == \"quit\":\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                article_id = int(article_id)\n",
    "                article = df.loc[df[\"id\"] == article_id, \"article\"].values[0]\n",
    "                break\n",
    "            except Exception:\n",
    "                print(\"Wrong id. Enter again: \")\n",
    "                continue\n",
    "        \n",
    "        if article_id == \"quit\":\n",
    "            break\n",
    "        \n",
    "        print(f\"Article id selected: {article_id}\")\n",
    "\n",
    "        # Ask user to enter the question\n",
    "        question = input(\"Please enter your question: \")\n",
    "        if question == \"quit\":\n",
    "            break\n",
    "        print(\"Question: \", question)\n",
    "\n",
    "        # Generate answer\n",
    "        answer = \"\"\n",
    "        if model_id == 1:\n",
    "            answer = answer_question_classic(question=question, article=article)\n",
    "        elif model_id == 2:\n",
    "            answer = predictor.predict_json({\n",
    "                        \"passage\": preprocess_article(article),\n",
    "                        \"question\": question\n",
    "                    })[\"best_span_str\"]\n",
    "        elif model_id == 3:\n",
    "            answer = qa_pipeline({\n",
    "                        \"context\": preprocess_article(article),\n",
    "                        \"question\": question\n",
    "                    })[\"answer\"]\n",
    "        elif model_id == 4:\n",
    "            answer = answer_question_bert_base(question=question, article=preprocess_article(article))\n",
    "        elif model_id == 5:\n",
    "            answer = answer_question_distilbert(question=question, article=preprocess_article(article))\n",
    "        elif model_id == 6:\n",
    "            answer = answer_question_bert_large(question=question, article=preprocess_article(article))\n",
    "\n",
    "        if answer == \"\":\n",
    "            answer = \"No answer found\"\n",
    "\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print()\n",
    "\n",
    "# Uncomment this line to run the app\n",
    "# start_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a91b4",
   "metadata": {},
   "source": [
    "## B. References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e45882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf7553c1",
   "metadata": {},
   "source": [
    "## C. Appendix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
